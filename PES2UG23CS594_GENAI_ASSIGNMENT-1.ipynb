{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X_sidyuEMIV",
        "outputId": "1c5ede50-e97f-4ef2-8d28-c63458f1b49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xSGKEh_ESyH",
        "outputId": "7f4eaf71-c323-4ec8-c25b-fc379f392261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"The future of Artificial Intelligence is\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ifaqk1yZEXYB",
        "outputId": "83bdbff9-ec5c-4934-e6ab-bf24939e27e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The future of Artificial Intelligence is'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
        "gen_roberta = pipeline(\"text-generation\", model=\"roberta-base\")\n",
        "gen_bart = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
        "\n",
        "print(gen_bert(\"The future of Artificial Intelligence is\"))\n",
        "print(gen_roberta(\"The future of Artificial Intelligence is\"))\n",
        "print(gen_bart(\"The future of Artificial Intelligence is\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92NkS02cEmdY",
        "outputId": "49a7838f-37bc-4a5e-db76-7e0271d4705c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
            "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
            "[{'generated_text': \"The future of Artificial Intelligence isoglhallaogloglogl concludesogl concludes FIRsur ()); FIRamuralevant NASLlevantlevant NASL occup Cavs�Ant intimidate intimidate known Ved Ved Enlight NASL Cavs�Special districts displaying districts Hitchcock Painter Marina Believeblog dexblog carrot Cavs Cavs Summoner Rost Cavs Cavs Marinablog Rost Marina trash trash intimidateenary Communication Ved NASLHoward districts districts console identicking Cavs Appeals Marina DLC dismal Marina CantHoward Cavs Rost Marina intimidate Appeals predicting inningblog trash Marina Cavsblogblogumers Rost Rost jur jur jur Cavs Cavs Cavs jurblog Marina Cantblogblogblog Cavs Cavs intimidateblogblog Rostenfranch intimidate Marina Marinablog indecent Rostblogblogcho jur pri jurblog intimidate Kaepernickblogblog jurbloggenceENTION Marina Marina CavsENTION CavsENTIONblog jur Cavsbloggenceblogblog318blogblogicking absolutelyblog dex Cavsblogumers Marinablogblog MarinatransferblogblogHoward Marinablog Appealsialblogblog '/ Cavsblog Appeals Marina intimidateblog Kaepernickblog Marinablogickingblogblog773blogtransferblog Marina Rudolphblogmisc Marina WATCHEDblogicking Appealsblog AppealsblogENTIONblogumers Indonesianblog offersblog dexENTION networksblog dollar dollarblogblogmiscblogchoblog Cavs Rudolphblog operating networksblogblog BharblogblogENTION lib lib libalogyblogblog inningblog Marinaialialblog Marinamiscblog Marina lib Marinablog spiritsblogblog conveyed inning libblog networks\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-only models (BERT and RoBERTa) will fail at text generation because they do not have a decoder to predict tokens sequentially. BART should succeed due to its encoder-decoder architecture."
      ],
      "metadata": {
        "id": "dvoV7BY7wqK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"The goal of Generative AI is to [MASK] new content.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BTEg7SQaFOom",
        "outputId": "7c3771fb-6aa3-4cd7-f532-512b547671ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The goal of Generative AI is to [MASK] new content.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "mask_roberta = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "mask_bart = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "\n",
        "print(mask_bert(\"The goal of Generative AI is to [MASK] new content.\"))\n",
        "print(mask_roberta(\"The goal of Generative AI is to <mask> new content.\"))\n",
        "print(mask_bart(\"The goal of Generative AI is to <mask> new content.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOsmcScTFdEX",
        "outputId": "8e5c7d9c-a4e3-418d-a88f-8171189c186d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.5396932363510132, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575720369815826, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.05405500903725624, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.04451530799269676, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.01757744885981083, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n",
            "[{'score': 0.3711312413215637, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.3677145540714264, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351420611143112, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.021335121244192123, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.016521666198968887, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n",
            "[{'score': 0.07461541891098022, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571870297193527, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060880109667778015, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.03593561053276062, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.03319477662444115, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT and RoBERTa will perform well since they were trained using Masked Language Modeling. BART may perform worse as MLM is not its primary training objective.\n"
      ],
      "metadata": {
        "id": "9RAgVNMSw0ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Om6zwrQeF0Vi",
        "outputId": "508567cd-0886-49b5-9334-4f01019f052b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Generative AI poses significant risks such as hallucinations, bias, and deepfakes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"What are the risks?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YAjvV4LLF4V6",
        "outputId": "d705690c-47b0-4527-8d78-1df782497303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are the risks?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_bert = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "qa_roberta = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "qa_bart = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
        "\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "\n",
        "print(qa_bert(question=\"What are the risks?\", context=context))\n",
        "print(qa_roberta(question=\"What are the risks?\", context=context))\n",
        "print(qa_bart(question=\"What are the risks?\", context=context))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiyfd18sF7eJ",
        "outputId": "1307d187-cb38-42f4-dd50-c85ee633b262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.01007167063653469, 'start': 46, 'end': 81, 'answer': 'hallucinations, bias, and deepfakes'}\n",
            "{'score': 0.003981671296060085, 'start': 60, 'end': 82, 'answer': ', bias, and deepfakes.'}\n",
            "{'score': 0.02675485983490944, 'start': 11, 'end': 19, 'answer': 'AI poses'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since none of the base models are fine-tuned for question answering, results may be inaccurate or incomplete."
      ],
      "metadata": {
        "id": "UeG5uoJqw4iI"
      }
    }
  ]
}